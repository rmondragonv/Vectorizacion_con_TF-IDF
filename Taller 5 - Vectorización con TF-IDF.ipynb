{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RicardoMondragon_Taller_5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLal4Rf27Td0"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_sp = stopwords.words('spanish')\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "spanishStemmer=SnowballStemmer(\"spanish\")\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_distances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7evIxDtc7pGg"
      },
      "source": [
        "bob = pd.read_csv(\"C:/Users/ricar/OneDrive/Documentos/Universidad/Konrad Lorenz/2o Semestre/Procesamiento de Lenguaje Natural/Taller 5/bob_esponja.csv\") \n",
        "bob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl5xpArI750U"
      },
      "source": [
        "#Quitar Mayusculas\n",
        "descripcion = bob[\"Descripción\"]\n",
        "texto = []\n",
        "outCaracters = []\n",
        "procesado = []\n",
        "for palabra in descripcion:\n",
        "  texto.append(palabra.lower())\n",
        "#Quitar caracteres especiales\n",
        "for palabra in texto:\n",
        "  palabra = re.sub(r\"[\\W\\d]\", \" \", palabra)\n",
        "  outCaracters.append(palabra.strip())\n",
        "#Stemming\n",
        "for palabra in outCaracters:\n",
        "  palabra = [word for word in palabra if word not in stopwords_sp]\n",
        "  palabra = [spanishStemmer.stem(word) for word in palabra]\n",
        "  procesado.append(palabra)\n",
        "\n",
        "procesado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K-Nwshz8QRl"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf = tfidf_vect.fit_transform(df[\"Procesado\"].values)\n",
        "\n",
        "tfidf_matrix = pd.DataFrame(tfidf.toarray(), columns=tfidf_vect.get_feature_names())\n",
        "tfidf_matrix.rename(columns={0:'Bob Esponja',1:'Patricio',2:'Calamardo',3:'Arenita',4:'Don Cangrejo',\n",
        "                             5:'Plankton',6:'Karen',7:'Perlita',8:'Sñ Puff',9:'Gary'}, inplace=True)\n",
        "tfidf_matrix\n",
        "\n",
        "tfidf_matrix.T.round(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ozGHSAh_chB"
      },
      "source": [
        "dist_cos = cosine_distances(tfidf_matrix.T.values)\n",
        "dist_cos = pd.DataFrame(dist_euc, columns = tfidf_matrix.columns, index = tfidf_matrix.columns)\n",
        "dist_cos\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdsbvskfDUTp"
      },
      "source": [
        "import seaborn as sns; sns.set_theme()\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3wVxsX4_pXr"
      },
      "source": [
        "## ¿Cuáles son los personajes más parecidos?\n",
        "\n",
        "Segun la distancia del coseno, podemos concluir que Don Cangrejo y Plankton son los personajes mas parecidos.\n",
        "\n",
        "## ¿Cuáles son los personajes más diferentes?\n",
        "\n",
        "Bob Esponja y Karen\n"
      ]
    }
  ]
}